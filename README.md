# California-Housing-Price-Prediction
Dataset: California Housing (20,640 samples, 8 features) Target: Median House Value (in hundreds of thousands) Goal: Predict house prices using various regression techniques

ğŸ“ˆ Key Findings
ğŸ† Best Performing Model

Winner: Polynomial Regression (degree=2)
Performance: RÂ² = 0.6457, RMSE = 0.6814
Improvement over baseline: +12.1% better than Linear Regression

ğŸ“Š Model Performance Ranking

Polynomial (degree=2) - RÂ² = 0.6457 â­ Best
Lasso Regression - RÂ² = 0.5769
Linear Regression - RÂ² = 0.5758 (Baseline)
Ridge Regression - RÂ² = 0.5758
RFE (5 features) - RÂ² = 0.5675
Huber Regressor - RÂ² = 0.5610
RANSAC - RÂ² = 0.3987
Theil-Sen - RÂ² = 0.2292

ğŸ” Technical Analysis
Multicollinearity Detection

High VIF features: Latitude (9.30), Longitude (8.96), AveRooms (8.34), AveBedrms (6.99)
Conclusion: Justified using Ridge/Lasso over plain OLS

Feature Importance (Linear Regression coefficients)

Latitude (-0.897) - Northern areas = lower prices
Longitude (-0.870) - Eastern areas = lower prices
MedInc (+0.854) - Higher income = higher prices
AveBedrms (+0.339) - More bedrooms = higher prices

Geographic Insights

Southern & Western California are most expensive
Income is the strongest positive predictor
Location (Lat/Long) dominates pricing

âœ… Proper train-test split (80-20)
âœ… Feature scaling with StandardScaler
âœ… VIF analysis for multicollinearity
âœ… Multiple regression techniques comparison
âœ… Residual analysis and assumption checking
âœ… Feature importance interpretation

Advanced Techniques

âœ… Polynomial feature expansion (8 â†’ 44 features)
âœ… Regularization (Ridge/Lasso with cross-validation)
âœ… Recursive Feature Elimination (RFE)
âœ… Robust regression methods (Huber, RANSAC, Theil-Sen)

ğŸ¯ Key Insights
Why Polynomial Won

Captured non-linear relationships between features
70% improvement in feature count (8 â†’ 44)
Better handling of interaction effects

Lasso Results

All 8 features retained (Î± = 0.001)
Indicates dataset is well-curated with minimal redundancy
Light regularization was optimal

#Results Quality

RÂ² = 0.6457 explains ~65% of price variance
RMSE = 0.6814 represents reasonable prediction error
Strong feature importance aligns with real-world expectations

ğŸš€ Analysis

Data preprocessing and scaling expertise
Understanding of regression assumptions
Knowledge of regularization techniques
Ability to interpret and compare models
Professional data science workflow

ğŸ‰ Conclusion
California Housing analysis was comprehensive, technically sound, and professionally executed. The polynomial regression result (RÂ² = 0.6457) represents predictive performance for this classic dataset. Successfully identified multicollinearity, applied appropriate techniques, and provided actionable insights about the California housing market.
